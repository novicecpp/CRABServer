{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946659",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('tape-recall-history')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "with open(secretpath, 'r') as r:\n",
    "    pass\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "FROM_DATE = os.environ.get('FROM_DATE', None) \n",
    "TO_DATE = os.environ.get('TO_DATE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import osearch from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    import osearch\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    import osearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook variables\n",
    "# modify value here when run inside notebook\n",
    "TODAY = \"2024-09-18\"\n",
    "YESTERDAY = (datetime.strptime(TODAY, \"%Y-%m-%d\") - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "index_name = 'crab-test-tape-recall-history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430146eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cronjob, replace constant with value from env\n",
    "if TO_DATE and FROM_DATE:\n",
    "    TODAY = TO_DATE\n",
    "    YESTERDAY = FROM_DATE\n",
    "if PROD:\n",
    "    index_name = f'crab-{\"-\".join(index_name.split(\"-\")[2:])}'\n",
    "# for osearch index pattern timestamp\n",
    "timestamp_str = datetime.strptime(TODAY, \"%Y-%m-%d\").strftime(\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "print(TODAY)\n",
    "print(YESTERDAY)\n",
    "print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85c2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data into spark\n",
    "\n",
    "HDFS_RUCIO_RULES_HISTORY = f'/project/awg/cms/rucio/{TODAY}/rules_history/'\n",
    "\n",
    "print(\"===============================================\"\n",
    "      , \"RUCIO : Rules History\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", HDFS_RUCIO_RULES_HISTORY\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')\n",
    "\n",
    "rucio_rules_history = spark.read.format('avro').load(HDFS_RUCIO_RULES_HISTORY).withColumn('ID', lower(_hex(col('ID'))))\n",
    "\n",
    "\n",
    "rucio_rules_history = rucio_rules_history.select(\"ID\", \"ACCOUNT\", \"NAME\", \"STATE\", \"EXPIRES_AT\", \"UPDATED_AT\", \"CREATED_AT\").filter(f\"\"\"ACTIVITY = 'Analysis TapeRecall'\"\"\").cache()\n",
    "rucio_rules_history.createOrReplaceTempView(\"rules_history\")\n",
    "\n",
    "HDFS_CRAB_part = f'/project/awg/cms/crab/tasks/{TODAY}/'\n",
    "print(\"===============================================\"\n",
    "      , \"CRAB Table\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", HDFS_CRAB_part\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')\n",
    "\n",
    "tasks_table = spark.read.format('avro').load(HDFS_CRAB_part)\n",
    "tasks_table = tasks_table.select(\"TM_TASKNAME\",\"TM_START_TIME\",\"TM_TASK_STATUS\",  'TM_TASKNAME', 'TM_START_TIME', 'TM_TASK_STATUS' , 'TM_DDM_REQID').cache()\n",
    "tasks_table.createOrReplaceTempView(\"tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271b1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query data in daily\n",
    "\n",
    "\n",
    "query = f\"\"\"\\\n",
    "WITH filter_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, EXPIRES_AT, UPDATED_AT, CREATED_AT\n",
    "FROM rules_history\n",
    "WHERE EXPIRES_AT < unix_timestamp(\"{TODAY} 00:00:00\", \"yyyy-MM-dd HH:mm:ss\")*1000\n",
    "), \n",
    "rn_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, EXPIRES_AT, UPDATED_AT, CREATED_AT,\n",
    "       row_number() over(partition by ID order by UPDATED_AT desc) as row_num\n",
    "FROM filter_t\n",
    "),\n",
    "latestupdate_t AS (\n",
    "SELECT * FROM rn_t \n",
    "WHERE row_num = 1\n",
    "),\n",
    "calc_days_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, EXPIRES_AT, UPDATED_AT, CREATED_AT,\n",
    "       CASE \n",
    "           WHEN STATE = 'O' THEN ceil((UPDATED_AT-CREATED_AT)/86400000)  \n",
    "           WHEN STATE != 'O' AND EXPIRES_AT < unix_timestamp(\"{TODAY} 00:00:00\", \"yyyy-MM-dd HH:mm:ss\")*1000 THEN ceil((EXPIRES_AT-CREATED_AT)/86400000)\n",
    "           ELSE 0\n",
    "       END AS DAYS\n",
    "FROM latestupdate_t\n",
    "),\n",
    "join_t AS (\n",
    "SELECT * FROM calc_days_t\n",
    "LEFT JOIN tasks ON calc_days_t.ID = tasks.TM_DDM_REQID\n",
    "),\n",
    "window_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, DAYS, EXPIRES_AT, UPDATED_AT, CREATED_AT, TM_TASKNAME, TM_START_TIME, TM_TASK_STATUS, \n",
    "       row_number() OVER (PARTITION BY join_t.ID ORDER BY join_t.TM_START_TIME DESC) AS row_num\n",
    "FROM join_t \n",
    "),\n",
    "uniqueid_t AS (\n",
    "SELECT *\n",
    "FROM window_t\n",
    "WHERE row_num =1\n",
    "), \n",
    "final_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, DAYS, EXPIRES_AT, UPDATED_AT, CREATED_AT, TM_TASKNAME, IFNULL(TM_START_TIME, 0) as TM_START_TIME, TM_TASK_STATUS, \n",
    "       CREATED_AT AS timestamp,\n",
    "       'crab_tape_recall_rules_history' AS type\n",
    "FROM uniqueid_t\n",
    ")\n",
    "--- set YESTERDAY to \"1970-01-01\" when recompute index to get all data from begining of time\n",
    "SELECT * FROM final_t\n",
    "WHERE 1=1\n",
    "AND EXPIRES_AT >= unix_timestamp(\"{YESTERDAY} 00:00:00\", \"yyyy-MM-dd HH:mm:ss\")*1000\n",
    "AND EXPIRES_AT < unix_timestamp(\"{TODAY} 00:00:00\", \"yyyy-MM-dd HH:mm:ss\")*1000 \n",
    "\"\"\"\n",
    "\n",
    "tmpdf = spark.sql(query)\n",
    "tmpdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tmpdf.toPandas().to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "        \"settings\": {\"index\": {\"number_of_shards\": \"1\", \"number_of_replicas\": \"1\"}},\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"ID\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"ACCOUNT\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"NAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"STATE\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"DAYS\": {\"type\": \"long\"},\n",
    "                \"EXPIRES_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"UPDATED_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"CREATED_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"TM_TASKNAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"TM_START_TIME\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"TM_TASK_STATUS\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"timestamp\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec824ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(osearch)\n",
    "osearch.send_os(docs, index_name, schema, secretpath, timestamp_str)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    },
    {
     "name": "spark.executor.instances",
     "value": "20"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
