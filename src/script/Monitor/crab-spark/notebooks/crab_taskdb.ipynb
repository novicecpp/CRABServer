{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcae07ec",
   "metadata": {},
   "source": [
    "# CRAB Spark taskdb\n",
    "\n",
    "This jobs will \"copy\" some column from TaskDB table to opensearch to answer theses questions:\n",
    "- How many tasks are using each crab features? (Split algorithm, Ignorelocality, ScriptExe, GPU)\n",
    "- How many tasks each users submit?\n",
    "- How many tasks use ignorelocality?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41c8e6",
   "metadata": {},
   "source": [
    "## Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_user,\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import osearch from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    import osearch\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    import osearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('crab-taskdb')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9013878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any cache left, for working with notebook\n",
    "# it safe to run everytime cronjob start\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6078f",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "\n",
    "We provide arguments to this script via env var. \n",
    "- `OPENSEARCH_SECRET_PATH`: path to secretfile, contain a line of <username>:<password> of opensearch that we send the data to\n",
    "- `PROD`: if true index prefix will be `crab-`, otherwise `crab-test-`\n",
    "- `START`: start date (YYYY-MM-dd)\n",
    "- `END`: end date (YYYY-MM-dd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "if not os.path.isfile(secretpath): \n",
    "    raise Exception(f'OS secrets file {secretpath} does not exists')\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "START = os.environ.get('START_DATE', None) \n",
    "END = os.environ.get('END_DATE', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e62ea",
   "metadata": {},
   "source": [
    "## Variables \n",
    "Will be used throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For run playbook manually, set start/end date here\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2024-10-01\"\n",
    "# if cronjob, replace constant with value from env\n",
    "if START and END:\n",
    "    START_DATE = START\n",
    "    END_DATE = END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index name\n",
    "index_name = 'taskdb'\n",
    "# use prod index pattern if this execution is for production\n",
    "if PROD:\n",
    "    index_name = f'crab-{index_name}'\n",
    "else:\n",
    "    index_name = f'crab-test-{index_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430146eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object\n",
    "start_datetime = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_datetime = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "# sanity check\n",
    "if end_datetime < start_datetime: \n",
    "    raise Exception(f\"end date ({END_DATE}) is less than start date ({START_DATE})\")\n",
    "start_epochmilis = int(start_datetime.timestamp()) * 1000\n",
    "end_epochmilis = int(end_datetime.timestamp()) * 1000\n",
    "yesterday_epoch = int((end_datetime-timedelta(days=1)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "print(START_DATE, \n",
    "      END_DATE, \n",
    "      index_name,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33ec96",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf35868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that \"today\" file, for example, today=2024-10-04, should be in directory /project/awg/cms/crab/tasks/2024-10-04 \n",
    "# which contain contents from the begining of table until the time of dump job run\n",
    "# which mean data before 2024-10-04 will be available, but not 2024-10-04 itself!\n",
    "\n",
    "HDFS_CRAB_part = f'/project/awg/cms/crab/tasks/{END_DATE}/' # data each day in hdfs contain whole table\n",
    "print(\"===============================================\"\n",
    "      , \"CRAB Table\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", HDFS_CRAB_part\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')\n",
    "\n",
    "tasks_df = spark.read.format('avro').load(HDFS_CRAB_part).cache()\n",
    "tasks_df = ( \n",
    "    tasks_df.select(\"TM_TASKNAME\",\"TM_START_TIME\",\"TM_TASK_STATUS\",\"TM_SPLIT_ALGO\",\"TM_USERNAME\",\"TM_USER_ROLE\",\"TM_JOB_TYPE\",\"TM_IGNORE_LOCALITY\",\"TM_SCRIPTEXE\",\"TM_USER_CONFIG\")\n",
    "             .filter(f\"\"\"\\\n",
    "                  1=1\n",
    "                  AND TM_START_TIME >= {start_epochmilis}\n",
    "                  AND TM_START_TIME < {end_epochmilis}\"\"\")\n",
    "             .cache()\n",
    ")\n",
    "tasks_df.createOrReplaceTempView(\"tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c634fe",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271b1c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\\\n",
    "WITH reqacc_tb AS (         \n",
    "SELECT TM_TASKNAME, TM_START_TIME, TM_TASK_STATUS, TM_SPLIT_ALGO, TM_USERNAME, TM_USER_ROLE, TM_JOB_TYPE, TM_IGNORE_LOCALITY, TM_SCRIPTEXE,\n",
    "       CASE \n",
    "           WHEN get_json_object(TM_USER_CONFIG, '$.requireaccelerator') = true THEN 'T'\n",
    "           ELSE 'F'\n",
    "       END AS REQUIRE_ACCELERATOR\n",
    "FROM tasks\n",
    "),\n",
    "finalize_tb AS (\n",
    "SELECT TM_TASKNAME, TM_START_TIME, TM_TASK_STATUS, TM_SPLIT_ALGO, TM_USERNAME, TM_USER_ROLE, TM_JOB_TYPE, TM_IGNORE_LOCALITY, TM_SCRIPTEXE, REQUIRE_ACCELERATOR,\n",
    "       TM_START_TIME AS timestamp,\n",
    "       'taskdb' AS type\n",
    "FROM reqacc_tb\n",
    ")\n",
    "SELECT * FROM finalize_tb\n",
    "\"\"\"\n",
    "\n",
    "tmpdf = spark.sql(query)\n",
    "tmpdf.show(10, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fc2e5",
   "metadata": {},
   "source": [
    "## Sending result to OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark df to dicts\n",
    "docs = tmpdf.toPandas().to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "            \"settings\": {\"index\": {\"number_of_shards\": \"1\", \"number_of_replicas\": \"1\"}},\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"TM_TASKNAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_START_TIME\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                    'TM_TASK_STATUS': {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_SPLIT_ALGO\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_USERNAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_USER_ROLE\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_JOB_TYPE\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_IGNORE_LOCALITY\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_SCRIPTEXE\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"REQUIRE_ACCELERATOR\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"type\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"timestamp\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec824ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload osearch in case the code is changed.\n",
    "# useful for playbook and safe when run as cron\n",
    "import importlib\n",
    "importlib.reload(osearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "osearch.send_os(docs, index_name, schema, secretpath, yesterday_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d03e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
