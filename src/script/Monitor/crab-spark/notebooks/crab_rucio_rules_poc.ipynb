{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_user,\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    "    split as _split,\n",
    "    regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22946659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 14:02:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://swan-gpu-t4-5x-jfkrw7kar3a2-node-1:30551\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_shell_swan</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9832359490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('crab-taskdb')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9013878",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "if not os.path.isfile(secretpath): \n",
    "    raise Exception(f'OS secrets file {secretpath} does not exists')\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "START = os.environ.get('START_DATE', None) \n",
    "END = os.environ.get('END_DATE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import osearch from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    import osearch\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    import osearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1680299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables for run inside notebook\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2024-10-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# const variable\n",
    "index_name = 'crab-test-ruio-rules' # always put test index prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430146eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cronjob, replace constant with value from env\n",
    "if START and END:\n",
    "    START_DATE = START\n",
    "    END_DATE = END\n",
    "# use prod index pattern if this execution is for production\n",
    "if PROD:\n",
    "    index_name = f'crab-{\"-\".join(index_name.split(\"-\")[2:])}'\n",
    "# datetime object\n",
    "start_datetime = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_datetime = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "if end_datetime < start_datetime:\n",
    "    raise Exception(f\"end date ({END_DATE}) is less than start date ({START_DATE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9404c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n",
      "2020-01-01 00:00:00+00:00\n",
      "2024-10-31\n",
      "2024-10-31 00:00:00+00:00\n",
      "crab-test-ruio-rules\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(START_DATE, \n",
    "      start_datetime, \n",
    "      END_DATE, \n",
    "      end_datetime, \n",
    "      index_name, \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e85c2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_df_rses(spark):\n",
    "    \"\"\"Get Spark dataframe of RSES\n",
    "    \"\"\"\n",
    "    hdfs_rses_path = '/project/awg/cms/rucio/{}/rses/part*.avro'.format(datetime.today().strftime('%Y-%m-%d'))\n",
    "    df_rses = spark.read.format(\"avro\").load(hdfs_rses_path) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('rse_tier', _split(col('RSE'), '_').getItem(0)) \\\n",
    "        .withColumn('rse_country', _split(col('RSE'), '_').getItem(1)) \\\n",
    "        .withColumn('rse_kind',\n",
    "                    when((col(\"rse\").endswith('Temp') | col(\"rse\").endswith('temp') | col(\"rse\").endswith('TEMP')),\n",
    "                         'temp')\n",
    "                    .when((col(\"rse\").endswith('Test') | col(\"rse\").endswith('test') | col(\"rse\").endswith('TEST')),\n",
    "                          'test')\n",
    "                    .otherwise('prod')\n",
    "                    ) \\\n",
    "        .select(['rse_id', 'RSE', 'RSE_TYPE', 'rse_tier', 'rse_country', 'rse_kind'])\n",
    "    return df_rses\n",
    "def get_df_locks(spark):\n",
    "    \"\"\"Get Spark dataframe of Locks\n",
    "    \"\"\"\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    locks_path = f'/project/awg/cms/rucio/{today}/locks/part*.avro'\n",
    "    locks = spark.read.format('avro').load(locks_path) \\\n",
    "                .filter(col('SCOPE') == 'cms') \\\n",
    "                .filter(col('STATE').isin(['O', 'R'])) \\\n",
    "                .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "                .withColumnRenamed('NAME', 'f_name') \\\n",
    "                .withColumnRenamed('ACCOUNT', 'account_name') \\\n",
    "                .withColumnRenamed('BYTES', 'f_size') \\\n",
    "                .withColumn('r_id', lower(_hex(col('RULE_ID')))) \\\n",
    "                .select(['rse_id', 'f_name', 'f_size', 'r_id', 'account_name'])\n",
    "    return locks\n",
    "def get_df_accounts(spark):\n",
    "    \"\"\"Get Spark dataframe of Accounts\n",
    "    \"\"\"\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    hdfs_rucio_accounts = f'/project/awg/cms/rucio/{today}/accounts/part*.avro'\n",
    "    df_accounts = spark.read.format(\"avro\").load(hdfs_rucio_accounts) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumnRenamed('ACCOUNT', 'account_name') \\\n",
    "        .withColumnRenamed('ACCOUNT_TYPE', 'account_type') \\\n",
    "        .select(['account_name', 'account_type'])\n",
    "    return df_accounts\n",
    "def get_df_rules(spark):\n",
    "    \"\"\"Get Spark dataframe of rules\n",
    "    \"\"\"\n",
    "    hdfs_rules_path = '/project/awg/cms/rucio/{}/rules/part*.avro'.format(datetime.today().strftime('%Y-%m-%d'))\n",
    "    return spark.read.format('avro').load(hdfs_rules_path) \\\n",
    "        .filter(col('SCOPE') == 'cms') \\\n",
    "        .withColumnRenamed('name', 'r_name') \\\n",
    "        .withColumn('r_id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('s_id', lower(_hex(col('SUBSCRIPTION_ID')))) \\\n",
    "        .withColumnRenamed('ACTIVITY', 'activity') \\\n",
    "        .withColumnRenamed('STATE', 'rule_state') \\\n",
    "        .withColumnRenamed('RSE_EXPRESSION', 'rse_expression') \\\n",
    "        .select(['r_name','r_id', 's_id', 'activity', 'rule_state', 'rse_expression']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e271b1c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modified\n",
    "df_rses = get_df_rses(spark)\n",
    "df_locks = get_df_locks(spark)\n",
    "df_accounts = get_df_accounts(spark)\n",
    "df_rules = get_df_rules(spark)\n",
    "tb_denominator = 10 ** 12\n",
    "locks = df_locks.join(df_rses, ['rse_id'], how='left') \\\n",
    "        .filter(col('rse_kind') == 'prod') \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'r_id']) \n",
    "\n",
    "locks_with_activity = (\n",
    "    locks.join(df_rules, ['r_id'], how='leftouter')\n",
    "         .select(['f_name', 'account_name', 'RSE', 'rse_type', 'f_size', 'activity', 'r_name'])\n",
    "         .withColumn('data_tier', regexp_extract('r_name', r'^\\/([\\w-]+)\\/([\\w-]+)\\/([\\w-]+)(#[\\w-]+)?', 3))\n",
    "         .select(['f_name', 'account_name', 'RSE', 'rse_type', 'f_size', 'activity', 'data_tier'])\n",
    ")\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# A File locked by the user for two activities is accounted to both activities\n",
    "# A File locked by two users for the same activity is accounted to both Users\n",
    "user_aggreagated = locks_with_activity \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'activity', 'data_tier']) \\\n",
    "        .distinct() \\\n",
    "        .groupby(['RSE', 'rse_type', 'account_name', 'activity', 'data_tier']) \\\n",
    "        .agg(_round(_sum(col('f_size')) / tb_denominator, 5).alias('total_locked')) \\\n",
    "        .join(df_accounts, ['account_name'], how='left') \\\n",
    "        .withColumnRenamed('RSE', 'rse_name') \\\n",
    "        .withColumn('timestamp', lit(timestamp)) \\\n",
    "        .select(['total_locked', 'rse_name', 'rse_type', 'account_name', 'account_type', 'activity', 'data_tier', 'timestamp']) \\\n",
    "        .cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15c3ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_194 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_74 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_22 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_96 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_53 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_84 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_48 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_196 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_10 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_22 !\n",
      "24/10/02 18:56:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_115 !\n",
      "24/10/02 18:56:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 111 for reason Container from a bad node: container_e289_1722958100713_123141_01_000128 on host: ithdp6018.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:34.788]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:34.829]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:34.830]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:34 ERROR YarnScheduler: Lost executor 111 on ithdp6018.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000128 on host: ithdp6018.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:34.788]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:34.829]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:34.830]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:34 WARN TaskSetManager: Lost task 64.0 in stage 172.0 (TID 4772) (ithdp6018.cern.ch executor 111): ExecutorLostFailure (executor 111 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000128 on host: ithdp6018.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:34.788]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:34.829]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:34.830]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:34 WARN TaskSetManager: Lost task 47.0 in stage 172.0 (TID 4708) (ithdp6018.cern.ch executor 111): ExecutorLostFailure (executor 111 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000128 on host: ithdp6018.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:34.788]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:34.829]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:34.830]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:34 WARN TaskSetManager: Lost task 62.0 in stage 172.0 (TID 4729) (ithdp6018.cern.ch executor 111): ExecutorLostFailure (executor 111 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000128 on host: ithdp6018.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:34.788]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:34.829]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:34.830]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_152 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_68 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_155 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_51 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_77 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_196 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_154 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_160 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_193 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_99 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_99 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_25 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_37 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_194 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_6 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_192 !\n",
      "24/10/02 18:56:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_0 !\n",
      "24/10/02 18:56:39 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 112 for reason Container from a bad node: container_e289_1722958100713_123141_01_000129 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:39.436]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:39.477]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:39.478]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:39 ERROR YarnScheduler: Lost executor 112 on ithdp7016.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000129 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:39.436]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:39.477]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:39.478]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:39 WARN TaskSetManager: Lost task 96.0 in stage 172.0 (TID 4834) (ithdp7016.cern.ch executor 112): ExecutorLostFailure (executor 112 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000129 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:39.436]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:39.477]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:39.478]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:39 WARN TaskSetManager: Lost task 58.0 in stage 172.0 (TID 4825) (ithdp7016.cern.ch executor 112): ExecutorLostFailure (executor 112 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000129 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:39.436]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:39.477]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:39.478]Killed by external signal\n",
      ".\n",
      "24/10/02 18:56:39 WARN TaskSetManager: Lost task 87.0 in stage 172.0 (TID 4830) (ithdp7016.cern.ch executor 112): ExecutorLostFailure (executor 112 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000129 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:56:39.436]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:56:39.477]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:56:39.478]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_142 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_184 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_144 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_112 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_114 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_110 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_147 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_83 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_21 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_143 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_1 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_84 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_182 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_177 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_134 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_135 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_10 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_36 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_109 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_136 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_52 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_62 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_111 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_191 !\n",
      "24/10/02 18:57:08 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_148 !\n",
      "24/10/02 18:57:08 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 128 for reason Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 ERROR YarnScheduler: Lost executor 128 on ithdp7011.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 WARN TaskSetManager: Lost task 77.0 in stage 175.0 (TID 4923) (ithdp7011.cern.ch executor 128): ExecutorLostFailure (executor 128 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 WARN TaskSetManager: Lost task 107.0 in stage 175.0 (TID 4953) (ithdp7011.cern.ch executor 128): ExecutorLostFailure (executor 128 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 WARN TaskSetManager: Lost task 17.0 in stage 175.0 (TID 4863) (ithdp7011.cern.ch executor 128): ExecutorLostFailure (executor 128 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:08 WARN TaskSetManager: Lost task 47.0 in stage 175.0 (TID 4893) (ithdp7011.cern.ch executor 128): ExecutorLostFailure (executor 128 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000146 on host: ithdp7011.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:08.167]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:08.208]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:08.209]Killed by external signal\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_49 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_3 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_49 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_80 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_97 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_75 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_18 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_23 !\n",
      "24/10/02 18:57:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_111 !\n",
      "24/10/02 18:57:24 ERROR YarnScheduler: Lost executor 108 on ithdp3107.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:24 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 108 for reason Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:24 WARN TaskSetManager: Lost task 93.0 in stage 175.0 (TID 4939) (ithdp3107.cern.ch executor 108): ExecutorLostFailure (executor 108 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:24 WARN TaskSetManager: Lost task 33.0 in stage 175.0 (TID 4879) (ithdp3107.cern.ch executor 108): ExecutorLostFailure (executor 108 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:24 WARN TaskSetManager: Lost task 63.0 in stage 175.0 (TID 4909) (ithdp3107.cern.ch executor 108): ExecutorLostFailure (executor 108 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:24 WARN TaskSetManager: Lost task 154.0 in stage 175.0 (TID 5004) (ithdp3107.cern.ch executor 108): ExecutorLostFailure (executor 108 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000125 on host: ithdp3107.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:23.977]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:24.017]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:24.018]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:33 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /188.184.195.43:21569 is closed\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_166 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_135 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_146 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_12 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_133 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_13 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_87 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_165 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_118 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_39 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_25 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_164 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_87 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_125 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_65 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_163 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_56 !\n",
      "24/10/02 18:57:33 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 125 from block manager BlockManagerId(110, ithdp7016.cern.ch, 5107, None)\n",
      "java.io.IOException: Connection from /188.184.195.43:21569 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 18:57:33 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 110 for reason Container from a bad node: container_e289_1722958100713_123141_01_000127 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:33.662]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:33.702]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:33.703]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:33 ERROR YarnScheduler: Lost executor 110 on ithdp7016.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000127 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:33.662]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:33.702]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:33.703]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:33 WARN TaskSetManager: Lost task 132.0 in stage 175.0 (TID 4982) (ithdp7016.cern.ch executor 110): ExecutorLostFailure (executor 110 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000127 on host: ithdp7016.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:33.662]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:33.702]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:33.703]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_119 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_29 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_88 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_55 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_26 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_57 !\n",
      "24/10/02 18:57:34 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_2 !\n",
      "24/10/02 18:57:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 126 for reason Container from a bad node: container_e289_1722958100713_123141_01_000142 on host: ithdp7001.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:34.247]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:34.288]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:34.289]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:34 ERROR YarnScheduler: Lost executor 126 on ithdp7001.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000142 on host: ithdp7001.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:34.247]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:34.288]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:34.289]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:34 WARN TaskSetManager: Lost task 170.0 in stage 175.0 (TID 5020) (ithdp7001.cern.ch executor 126): ExecutorLostFailure (executor 126 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000142 on host: ithdp7001.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:34.247]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:34.288]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:34.289]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_108 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_46 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_15 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_83 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_285_11 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_61 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_77 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_8 !\n",
      "24/10/02 18:57:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_35 !\n",
      "24/10/02 18:57:38 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 120 for reason Container from a bad node: container_e289_1722958100713_123141_01_000137 on host: ithdp3101.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:38.395]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:38.436]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:38.437]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:38 ERROR YarnScheduler: Lost executor 120 on ithdp3101.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000137 on host: ithdp3101.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:38.395]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:38.436]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:38.437]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:38 WARN TaskSetManager: Lost task 185.0 in stage 175.0 (TID 5035) (ithdp3101.cern.ch executor 120): ExecutorLostFailure (executor 120 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000137 on host: ithdp3101.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:38.395]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:38.436]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:38.437]Killed by external signal\n",
      ".\n",
      "24/10/02 18:57:38 WARN TaskSetManager: Lost task 191.0 in stage 175.0 (TID 5041) (ithdp3101.cern.ch executor 120): ExecutorLostFailure (executor 120 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000137 on host: ithdp3101.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 18:57:38.395]Container killed on request. Exit code is 137\n",
      "[2024-10-02 18:57:38.436]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 18:57:38.437]Killed by external signal\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+--------+--------------------+------------+------------------+------------------------+----------+\n",
      "|total_locked|rse_name           |rse_type|account_name        |account_type|activity          |data_tier               |timestamp |\n",
      "+------------+-------------------+--------+--------------------+------------+------------------+------------------------+----------+\n",
      "|351.10199   |T0_CH_CERN_Tape    |TAPE    |sync_t0_ch_cern_tape|USER        |Data Consolidation|USER                    |1727888173|\n",
      "|119.3932    |T2_IT_Legnaro      |DISK    |transfer_ops        |SERVICE     |Data Consolidation|MINIAODSIM              |1727888173|\n",
      "|4.93308     |T2_BE_IIHE         |DISK    |wma_prod            |SERVICE     |Production Output |ALCARECO                |1727888173|\n",
      "|3.33033     |T3_KR_KISTI        |DISK    |geonmo              |USER        |Data Consolidation|NANOAODSIM              |1727888173|\n",
      "|9.82887     |T2_RU_IHEP         |DISK    |transfer_ops        |SERVICE     |Production Output |NANOAODSIM              |1727888173|\n",
      "|68.68596    |T1_DE_KIT_Tape     |TAPE    |transfer_ops        |SERVICE     |User Subscriptions|GEN-SIM-DIGI-RAW        |1727888173|\n",
      "|51.17587    |T1_UK_RAL_Tape     |TAPE    |sync_t1_uk_ral_tape |USER        |Data Consolidation|GEN-SIM-DIGI-RAW-MINIAOD|1727888173|\n",
      "|3.34256     |T2_BE_UCL          |DISK    |aguzel              |USER        |User Subscriptions|NANOAODSIM              |1727888173|\n",
      "|48.4789     |T1_US_FNAL_Disk    |DISK    |crab_input          |SERVICE     |Analysis Input    |AODSIM                  |1727888173|\n",
      "|2.09268     |T2_UK_SGrid_Bristol|DISK    |wma_prod            |SERVICE     |Production Output |MINIAODSIM              |1727888173|\n",
      "+------------+-------------------+--------+--------------------+------------+------------------+------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_aggreagated.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7e98534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6578"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_aggreagated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26a7355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "tb_denominator = 10 ** 12\n",
    "  \n",
    "\n",
    "df_rses = get_df_rses(spark)\n",
    "df_locks = get_df_locks(spark)\n",
    "df_accounts = get_df_accounts(spark)\n",
    "df_rules = get_df_rules(spark)\n",
    "\n",
    "locks = df_locks.join(df_rses, ['rse_id'], how='left') \\\n",
    "        .filter(col('rse_kind') == 'prod') \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'r_id']) \n",
    "\n",
    "locks_with_activity = locks.join(df_rules, ['r_id'], how='leftouter').select(['f_name', 'account_name', 'RSE', 'rse_type', 'f_size', 'activity'])\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# A File locked by the user for two activities is accounted to both activities\n",
    "# A File locked by two users for the same activity is accounted to both Users\n",
    "user_aggreagated = locks_with_activity \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'activity']) \\\n",
    "        .distinct() \\\n",
    "        .groupby(['RSE', 'rse_type', 'account_name', 'activity']) \\\n",
    "        .agg(_round(_sum(col('f_size')) / tb_denominator, 5).alias('total_locked')) \\\n",
    "        .join(df_accounts, ['account_name'], how='left') \\\n",
    "        .withColumnRenamed('RSE', 'rse_name') \\\n",
    "        .withColumn('timestamp', lit(timestamp)) \\\n",
    "        .select(['total_locked', 'rse_name', 'rse_type', 'account_name', 'account_type', 'activity', 'timestamp']) \\\n",
    "        .cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d75860da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 19:35:19 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /188.184.195.43:34778 is closed\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_134 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_30 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_98 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_124 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_198 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_126 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_327_27 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_3 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_199 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_5 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_56 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_67 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_36 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 144 from block manager BlockManagerId(107, ithdp7003.cern.ch, 5110, None)\n",
      "java.io.IOException: Connection from /188.184.195.43:34778 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_128 !\n",
      "24/10/02 19:35:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_193 !\n",
      "24/10/02 19:35:19 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 107 for reason Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:35:19 ERROR YarnScheduler: Lost executor 107 on ithdp7003.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:35:19 WARN TaskSetManager: Lost task 10.0 in stage 206.0 (TID 5627) (ithdp7003.cern.ch executor 107): ExecutorLostFailure (executor 107 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:35:19 WARN TaskSetManager: Lost task 41.0 in stage 206.0 (TID 5640) (ithdp7003.cern.ch executor 107): ExecutorLostFailure (executor 107 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:35:19 WARN TaskSetManager: Lost task 8.0 in stage 206.0 (TID 5580) (ithdp7003.cern.ch executor 107): ExecutorLostFailure (executor 107 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:35:19 WARN TaskSetManager: Lost task 9.0 in stage 206.0 (TID 5607) (ithdp7003.cern.ch executor 107): ExecutorLostFailure (executor 107 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000122 on host: ithdp7003.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:35:19.678]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:35:19.719]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:35:19.719]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_86 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_64 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_12 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_123 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_61 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_30 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_38 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_327_30 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_123 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_183 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_154 !\n",
      "24/10/02 19:36:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_238_92 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/02 19:36:02 ERROR YarnScheduler: Lost executor 124 on ithdp7007.cern.ch: Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 124 for reason Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN TaskSetManager: Lost task 126.0 in stage 209.0 (TID 5846) (ithdp7007.cern.ch executor 124): ExecutorLostFailure (executor 124 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN TaskSetManager: Lost task 125.0 in stage 209.0 (TID 5845) (ithdp7007.cern.ch executor 124): ExecutorLostFailure (executor 124 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN TaskSetManager: Lost task 127.0 in stage 209.0 (TID 5847) (ithdp7007.cern.ch executor 124): ExecutorLostFailure (executor 124 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n",
      "24/10/02 19:36:02 WARN TaskSetManager: Lost task 124.0 in stage 209.0 (TID 5844) (ithdp7007.cern.ch executor 124): ExecutorLostFailure (executor 124 exited caused by one of the running tasks) Reason: Container from a bad node: container_e289_1722958100713_123141_01_000141 on host: ithdp7007.cern.ch. Exit status: 137. Diagnostics: [2024-10-02 19:36:02.629]Container killed on request. Exit code is 137\n",
      "[2024-10-02 19:36:02.669]Container exited with a non-zero exit code 137. \n",
      "[2024-10-02 19:36:02.670]Killed by external signal\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1546"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_aggreagated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e19391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------------------------------+------------+\n",
      "|rse_id                          |f_name                                                                                                                                                                                                           |f_size    |r_id                            |account_name|\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------------------------------+------------+\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/40001/F91BF69D-5A9E-5549-99A7-D95171672BE4.root                                  |3378174080|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/10000/047449AA-0B7E-F749-9AD9-48D345D9A567.root                                  |3367614511|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "|a6981dee6ed14b7c8c1b0e9fe7644401|/store/mc/RunIISummer16NanoAODv6/MonoHToWWToLNujj_2HDMa_TuneCUETP8M1_13TeV_madgraph-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_rp_102X_mcRun2_asymptotic_v7-v1/240000/4C3C97FB-9849-A947-B582-8F25A6F21DD7.root|59692327  |61fab85eee1648d896be00c803d9ec71|transfer_ops|\n",
      "|a0c1b4ce18324a9d80ea60f16e8b485d|/store/mc/RunIISummer16NanoAODv6/MonoHToWWToLNujj_2HDMa_TuneCUETP8M1_13TeV_madgraph-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_rp_102X_mcRun2_asymptotic_v7-v1/240000/4C3C97FB-9849-A947-B582-8F25A6F21DD7.root|59692327  |61fab85eee1648d896be00c803d9ec71|transfer_ops|\n",
      "|751eb808adf54eada3be77dd40c9c399|/store/mc/RunIISummer16NanoAODv6/MonoHToWWToLNujj_2HDMa_TuneCUETP8M1_13TeV_madgraph-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_rp_102X_mcRun2_asymptotic_v7-v1/240000/4C3C97FB-9849-A947-B582-8F25A6F21DD7.root|59692327  |d84d0f1f61484a668daa38284d35d9e7|transfer_ops|\n",
      "|745838195c1c42fe9d95b3d1905c1f50|/store/mc/RunIISummer16NanoAODv6/MonoHToWWToLNujj_2HDMa_TuneCUETP8M1_13TeV_madgraph-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_rp_102X_mcRun2_asymptotic_v7-v1/240000/4C3C97FB-9849-A947-B582-8F25A6F21DD7.root|59692327  |15cac764db10412ca56c923f458a5d51|transfer_ops|\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/10000/0660FB5C-C77B-1C47-BE51-07AB14D5E108.root                                  |3360941245|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/10000/1610EE25-C55B-D843-ADDD-84B49165416A.root                                  |3376347821|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/10000/21FA4475-314F-484D-B39F-E2489B5B8161.root                                  |3343697025|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "|5017683aea934c46b94bac226f086e53|/store/mc/RunIIFall18wmLHEGS/SUSYGluGluToBBHToBB_M-400_TuneCP5_13TeV-amcatnlo-pythia8/GEN-SIM/102X_upgrade2018_realistic_v11-v1/10000/27F9A055-C47F-2249-AA30-5377972F2BB6.root                                  |3349052551|d49f42ec5f024182a4e2d64d4823824b|transfer_ops|\n",
      "+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_locks.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce94acc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2363240"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rules.filter(~col('r_name').contains('#')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fa40f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+----+------------------+----------+-------------------+----------+\n",
      "|r_name                                                                                                                                                                                                   |r_id                            |s_id|activity          |rule_state|rse_expression     |newcol    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+----+------------------+----------+-------------------+----------+\n",
      "|/BulkGravTohhTohtatahbb_narrow_M-1800_13TeV-madgraph/RunIISummer16DR80Premix-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6-v1/AODSIM#12711aac-bc43-11e6-9206-001e67abefa8                          |2ed1695632834e858a79d9f6708ed5aa|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|AODSIM    |\n",
      "|/BBbarDMJets_pseudo_LO_Mchi-10_Mphi-50_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIISummer16MiniAODv3-PUMoriond17_94X_mcRun2_asymptotic_v3-v2/MINIAODSIM#a58d7b45-a638-45c9-a2cc-70516e174732                |862fb12ea73b4a869cf81fe751e2e3c5|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|MINIAODSIM|\n",
      "|/BsToJpsiPhi_MuonFilter_SoftQCDnonD_TuneCP5_13TeV-pythia8-evtgen/RunIIFall17DRPremix-PU2017_94X_mc2017_realistic_v11-v1/AODSIM#01404956-008c-43e3-baa2-d79cd4854546                                      |3c27550fb7bb44e0ac3e523333ed8909|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|AODSIM    |\n",
      "|/BBbarDMJets_pseudo_LO_Mchi-50_Mphi-300_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIISummer16MiniAODv2-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6-v1/MINIAODSIM#e75e6d12-d090-11e6-933c-002590494c06|498dfebb54e24147bea4b9eddeb5b416|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|MINIAODSIM|\n",
      "|/BuToMuMuPi_BMuonFilter_SoftQCDnonD_TuneCP5_14TeV-pythia8-evtgen/Run3Summer19DRPremix-2023Scenario_106X_mcRun3_2023_realistic_v3-v1/AODSIM#3614c87d-ab25-44dc-9613-1191b86f8b0d                          |3ba52908db9d45128f209a389559f9c1|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|AODSIM    |\n",
      "|/BBbarDMJets_scalar_LO_Mchi-40_Mphi-100_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIISummer16MiniAODv2-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6-v1/MINIAODSIM#05d8ec0a-d153-11e6-8f74-001e67abef8c|098d7274b0fe4f6db742735506561db4|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|MINIAODSIM|\n",
      "|/BBbarDMJets_scalar_LO_Mchi-100_Mphi-500_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIISummer16DR80Premix-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6-v1/AODSIM#02d03a14-d026-11e6-933c-002590494c06  |782dff58e85e464cb02ed873935321a3|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|AODSIM    |\n",
      "|/BprimeTToHB_M-700_RH_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIIFall15MiniAODv1-PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1/MINIAODSIM#3d622908-9f35-11e5-943a-001e67abf518                             |ef3bba9f1b1644a0b3f0c8fb6a20d6f2|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|MINIAODSIM|\n",
      "|/BuToJpsiK_BMuonFilter_TuneCP5_13TeV-pythia8-evtgen/RunIIFall17MiniAODv2-PU2017_12Apr2018_94X_mc2017_realistic_v14-v1/MINIAODSIM#bc3b6ae7-8871-44f5-b36c-afe0c1fb14cd                                    |fbaac714cd414bb2a3764310caea73f3|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|MINIAODSIM|\n",
      "|/BBbarDMJets_scalar_NLO_Mchi-50_Mphi-200_TuneCUETP8M1_13TeV-madgraph-pythia8/RunIISummer15wmLHEGS-MCRUN2_71_V1-v2/LHE#5f49de04-d77d-11e6-8f74-001e67abef8c                                               |2d2bc6bff4194bce89d21cd5dcd72edf|NULL|Data Consolidation|O         |rse=T1_IT_CNAF_Tape|LHE       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+----+------------------+----------+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rules.select(\"*\").withColumn('newcol', regexp_extract('r_name', r'^\\/([\\w-]+)\\/([\\w-]+)\\/([\\w-]+)(#[\\w-]+)?', 3)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca8a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    },
    {
     "name": "spark.executor.instances",
     "value": "20"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
