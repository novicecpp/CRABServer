{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecefbb5",
   "metadata": {},
   "source": [
    "# CRAB Spark tape recall history\n",
    "\n",
    "This jobs is querying `rules_history` table of cmsrucio to answer theses questions:\n",
    "- How long do tasks stay in “taperecall”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_user,\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946659",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('tape-recall-history')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "if not os.path.isfile(secretpath): \n",
    "    raise Exception(f'OS secrets file {secretpath} does not exists')\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "START = os.environ.get('START_DATE', None) \n",
    "END = os.environ.get('END_DATE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import osearch from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    import osearch\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    import osearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c644790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for run inside notebook\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2024-10-01\"\n",
    "# if cronjob, replace constant with value from env\n",
    "if START and END:\n",
    "    START_DATE = START\n",
    "    END_DATE = END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index name\n",
    "index_name = 'tape-recall-history' # always put test index prefix\n",
    "# use prod index pattern if this execution is for production\n",
    "if PROD:\n",
    "    index_name = f'crab-prod-{index_name}'\n",
    "else:\n",
    "    index_name = f'crab-test-{index_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object\n",
    "start_datetime = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_datetime = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "# sanity check\n",
    "if end_datetime < start_datetime: \n",
    "    raise Exception(f\"end date ({END_DATE}) is less than start date ({START_DATE})\")\n",
    "start_epochmilis = int(start_datetime.timestamp()) * 1000\n",
    "end_epochmilis = int(end_datetime.timestamp()) * 1000\n",
    "yesterday_epoch = int((end_datetime-timedelta(days=1)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "print(START_DATE, \n",
    "      END_DATE, \n",
    "      index_name,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85c2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data into spark\n",
    "\n",
    "HDFS_RUCIO_RULES_HISTORY = f'/project/awg/cms/rucio/{END_DATE}/rules_history/'\n",
    "\n",
    "print(\"===============================================\"\n",
    "      , \"RUCIO : Rules History\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", HDFS_RUCIO_RULES_HISTORY\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')\n",
    "\n",
    "# we only interest in the rules where state does not change anymore.\n",
    "# which means, only the rules that already expired.\n",
    "rucio_rules_history = (\n",
    "    spark.read.format('avro').load(HDFS_RUCIO_RULES_HISTORY).withColumn('ID', lower(_hex(col('ID'))))\n",
    "         .select(\"ID\", \"ACCOUNT\", \"NAME\", \"STATE\", \"EXPIRES_AT\", \"UPDATED_AT\", \"CREATED_AT\")\n",
    "         .filter(f\"\"\"\\\n",
    "                  1=1\n",
    "                  AND ACTIVITY = 'Analysis TapeRecall'\n",
    "                  AND EXPIRES_AT >= {start_epochmilis}\n",
    "                  AND EXPIRES_AT < {end_epochmilis}\n",
    "                  \"\"\")\n",
    "         .cache()\n",
    ")\n",
    "rucio_rules_history.createOrReplaceTempView(\"rules_history\")\n",
    "\n",
    "HDFS_CRAB_part = f'/project/awg/cms/crab/tasks/{END_DATE}/'\n",
    "print(\"===============================================\"\n",
    "      , \"CRAB Table\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", HDFS_CRAB_part\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')\n",
    "\n",
    "# do not filter taskdb by create time (TM_START_TIME) because it is possible that rules are created 6 months ago\n",
    "tasks_df = (\n",
    "    spark.read.format('avro').load(HDFS_CRAB_part)\n",
    "         .select(\"TM_TASKNAME\",\"TM_START_TIME\",\"TM_TASK_STATUS\",  'TM_TASKNAME', 'TM_START_TIME', 'TM_TASK_STATUS' , 'TM_DDM_REQID')\n",
    "         .cache()\n",
    ")\n",
    "tasks_df.createOrReplaceTempView(\"tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rucio append new row to rules_history when the content rules table change (not sure the exact condition)\n",
    "# We need to get \"the latest\" row for each rules by:\n",
    "# - If rule has state \"O\", select the earliest UPDATED_AT row.\n",
    "#   For the OK rule, we can calculate number of days using UPDATED_AT-CREATED_AT. \n",
    "#   However, there are some posiblility that rucio append new entry with newer UPDATED_AT (For exmple 37fcada73f14439b88558ef792e10276)\n",
    "# - If not, select the latest UPDATED_AT row.\n",
    "#   This because the rules still in temporary state, and the rules will go to the end state \n",
    "#   (not the real state, but rules_history will not getting new row anymore) after rules is expired \n",
    "#   So, we can calculate number of day by EXPIRES_AT-CREATED_AT\n",
    "#\n",
    "# Here is the step to translate above condition to SQL (in the buttom-up manner)\n",
    "# 1. count number of row where the state is 'O'.\n",
    "# 2. left join the rule history by ID, so each row will have number of state O \n",
    "#    New table look like this:\n",
    "#    +--------------------------------+-----+-------------+-------+\n",
    "#    |ID                              |STATE|EXPIRES_AT   |state_o|\n",
    "#    +--------------------------------+-----+-------------+-------+\n",
    "#    |6d275222b43d431abc568dd83313118f|R    |1727244523000|1      |\n",
    "#    |875a388ca374407ea761689511078956|R    |1727339056000|1      |\n",
    "#    |dfe4012bcb9c448f98f940f01302ae6e|R    |1727234937000|0      |\n",
    "#    |dfe4012bcb9c448f98f940f01302ae6e|R    |1725402537000|0      |\n",
    "#    |c6859b18a771440ab906733e2bebf78a|R    |1727235038000|1      |\n",
    "#     \n",
    "# 3. select the earliest row for \"the rule that have state O\" (where clause). this can be done by windows function, sort by UPDATED_AT ascending for each ID, then filter only row_number \"1\"\n",
    "# 4. select the latest row for \"the rule that does not have state O at all\". \n",
    "#    This is a bit tricky but can be done by filter out the rule that have number of state O more than zero.\n",
    "#    which this column already availabe from left join in step 2.\n",
    "#    For the \"select latest row\" we do the same way as 4. but sort by UPDATED_AT descending instead.\n",
    "# 5. merge result from 3. and 4 with UNION ALL.\n",
    "# 6. Then, we will calculate number of date in the next step\n",
    "#\n",
    "# We are selecting the rules for each condition and join later, to avoid large broadcasthashjoin internally\n",
    "# I (Wa) tried this before and it cause above issue, but I might be wrong here though.\n",
    "# ```\n",
    "#  SELECT * FROM rhistinfo_t \n",
    "#  WHERE (state_o > 0) \n",
    "#    OR (ID NOT IN (SELECT ID FROM (SELECT * FROM rhistinfo_t WHERE state_o > 0)))\n",
    "# ```\n",
    "# \n",
    "\n",
    "query = f\"\"\"\\\n",
    "WITH \n",
    "count_t AS (\n",
    "SELECT ID, \n",
    "       SUM(CASE WHEN state = 'O' THEN 1 ELSE 0 END) AS state_o\n",
    "FROM rules_history\n",
    "GROUP BY ID\n",
    "),\n",
    "rhistinfo_t AS (\n",
    "SELECT rules_history.ID AS ID, \n",
    "       rules_history.ACCOUNT AS ACCOUNT, \n",
    "       rules_history.NAME AS NAME, \n",
    "       rules_history.STATE AS STATE, \n",
    "       rules_history.EXPIRES_AT AS EXPIRES_AT, \n",
    "       rules_history.UPDATED_AT AS UPDATED_AT, \n",
    "       rules_history.CREATED_AT AS CREATED_AT,\n",
    "       count_t.state_o AS state_o\n",
    "FROM rules_history\n",
    "LEFT JOIN count_t ON rules_history.ID = count_t.ID\n",
    "),\n",
    "tmpwindow_1 AS (\n",
    "SELECT *, row_number() over(partition by ID order by UPDATED_AT) as row_num\n",
    "FROM rhistinfo_t\n",
    "WHERE STATE = 'O'\n",
    "), \n",
    "r1 AS (\n",
    "SELECT * FROM tmpwindow_1\n",
    "WHERE row_num = 1\n",
    "),\n",
    "tmpwindow_2 AS (\n",
    "SELECT *, row_number() over(partition by ID order by UPDATED_AT DESC) as row_num\n",
    "FROM rhistinfo_t\n",
    "WHERE STATE != 'O' AND state_o = 0\n",
    "),\n",
    "r2 AS (\n",
    "SELECT * FROM tmpwindow_2\n",
    "WHERE row_num = 1\n",
    "),\n",
    "r_all AS (\n",
    "SELECT * FROM r1\n",
    "UNION ALL\n",
    "SELECT * FROM r2\n",
    ")\n",
    "SELECT * \n",
    "FROM r_all\n",
    "ORDER BY ID\n",
    "\"\"\"\n",
    "\n",
    "tmprules = spark.sql(query)\n",
    "tmprules.show(10, False)\n",
    "tmprules.createOrReplaceTempView(\"tmprules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of days, for state O, UPDATED_AT-CREATED_AT, otherwise EXPIRES_AT-CREATED_AT\n",
    "# then enrich the data with the crab taskdb table by join rule ID with TM_DDM_REQID column\n",
    "# need to apply windows function again to select only the rule id with the latest crab tasks\n",
    "\n",
    "query = f\"\"\"\\\n",
    "WITH \n",
    "calc_days_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, EXPIRES_AT, UPDATED_AT, CREATED_AT,\n",
    "       CASE \n",
    "           WHEN STATE = 'O' THEN ceil((UPDATED_AT-CREATED_AT)/86400000)  \n",
    "           ELSE ceil((EXPIRES_AT-CREATED_AT)/86400000)\n",
    "       END AS DAYS\n",
    "FROM tmprules\n",
    "),\n",
    "join_t AS (\n",
    "SELECT \n",
    "    calc_days_t.ID AS ID, \n",
    "    calc_days_t.ACCOUNT AS ACCOUNT, \n",
    "    calc_days_t.NAME AS NAME, \n",
    "    calc_days_t.STATE AS STATE, \n",
    "    calc_days_t.DAYS AS DAYS, \n",
    "    calc_days_t.EXPIRES_AT AS EXPIRES_AT, \n",
    "    calc_days_t.UPDATED_AT AS UPDATED_AT, \n",
    "    calc_days_t.CREATED_AT AS CREATED_AT, \n",
    "    tasks.TM_TASKNAME AS TM_TASKNAME,\n",
    "    IFNULL(tasks.TM_START_TIME, 0) AS TM_START_TIME, \n",
    "    tasks.TM_TASK_STATUS AS TM_TASK_STATUS\n",
    "FROM calc_days_t\n",
    "LEFT JOIN tasks ON calc_days_t.ID = tasks.TM_DDM_REQID\n",
    "),\n",
    "window_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, DAYS, EXPIRES_AT, UPDATED_AT, CREATED_AT, TM_TASKNAME, TM_START_TIME, TM_TASK_STATUS, \n",
    "       row_number() OVER (PARTITION BY ID ORDER BY TM_START_TIME DESC) AS row_num\n",
    "FROM join_t \n",
    "),\n",
    "uniqueid_t AS (\n",
    "SELECT *\n",
    "FROM window_t \n",
    "WHERE row_num = 1\n",
    "), \n",
    "finalize_t AS (\n",
    "SELECT ID, ACCOUNT, NAME, STATE, DAYS, EXPIRES_AT, UPDATED_AT, CREATED_AT, TM_TASKNAME, IFNULL(TM_START_TIME, 0) as TM_START_TIME, TM_TASK_STATUS, \n",
    "       EXPIRES_AT AS timestamp,\n",
    "       'tape_recall_history' AS type\n",
    "FROM uniqueid_t \n",
    ")\n",
    "SELECT *\n",
    "FROM finalize_t\n",
    "\"\"\"\n",
    "\n",
    "tmpdf = spark.sql(query)\n",
    "tmpdf.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df979012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tmpdf.toPandas().to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "        \"settings\": {\"index\": {\"number_of_shards\": \"1\", \"number_of_replicas\": \"1\"}},\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"ID\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"ACCOUNT\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"NAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"STATE\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"DAYS\": {\"type\": \"long\"},\n",
    "                \"EXPIRES_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"UPDATED_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"CREATED_AT\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"TM_TASKNAME\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"TM_START_TIME\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                \"TM_TASK_STATUS\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"type\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                \"timestamp\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec824ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(osearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "osearch.send_os(docs, index_name, schema, secretpath, yesterday_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22747a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a single doc to es everyday to check if pipeline is running successfully.\n",
    "# This is need because we did not have rule that expires everyday\n",
    "# Remember to filter it out in grafana (For example `NOT ID:00000000000000000` in lucene query)\n",
    "day = start_datetime\n",
    "monitoring_docs = []\n",
    "while day < end_datetime:\n",
    "    milisec = int(day.timestamp())*1000\n",
    "    doc = {\n",
    "        \"ID\": '00000000000000000',\n",
    "        \"ACCOUNT\": 'cmscrab',\n",
    "        \"NAME\": '/Pipeline/Monitoring/AOD',\n",
    "        \"STATE\": 'P',\n",
    "        \"DAYS\": -1,\n",
    "        \"EXPIRES_AT\": milisec,\n",
    "        \"UPDATED_AT\": milisec,\n",
    "        \"CREATED_AT\": milisec,\n",
    "        \"TM_TASKNAME\": '240000_000000:cmscrab_crab_20240000_000000',\n",
    "        \"TM_START_TIME\": milisec,\n",
    "        \"TM_TASK_STATUS\": 'PLACEHOLDER',\n",
    "        \"type\": 'tape_recall_history',\n",
    "        \"timestamp\": milisec,\n",
    "\n",
    "    }\n",
    "    monitoring_docs.append(doc)\n",
    "    day += timedelta(days=1)\n",
    "osearch.send_os(monitoring_docs, index_name, schema, secretpath, yesterday_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful query to get only the rules that gave\n",
    "#query = f\"\"\"\\\n",
    "#repeated_ids AS (\n",
    "#    SELECT ID\n",
    "#    FROM rules_history\n",
    "#    GROUP BY ID\n",
    "#    HAVING COUNT(*) > 2\n",
    "#),\n",
    "#tba_t AS (\n",
    "#SELECT *\n",
    "#FROM rules_history\n",
    "#)\n",
    "#SELECT * FROM tba_t\n",
    "#\"\"\"\n",
    "#\n",
    "#testdf = spark.sql(query)\n",
    "#testdf.show(100, False)\n",
    "#\n",
    "# rule 37fc where latest UPDATED_AT is 43 days after the first OK state\n",
    "#spark.sql(\"\"\"\\\n",
    "#SELECT * FROM rules_history\n",
    "#WHERE ID = '37fcada73f14439b88558ef792e10276'\n",
    "#\"\"\").show(10, False)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    },
    {
     "name": "spark.executor.instances",
     "value": "20"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
